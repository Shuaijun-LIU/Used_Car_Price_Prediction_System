{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1. **Text Extraction**:\n",
    "   - The 'Text' field is extracted from each item in the loaded data, which will be used for subsequent natural language processing tasks.\n",
    "\n",
    "2. **Preprocessing and NLP Operations**:\n",
    "   - **Stopword Removal**: Common stopwords are removed from the text using the `remove_stopwords` function from `gensim`.\n",
    "   - **Stemming**: Texts are processed using the `PorterStemmer` from `nltk` to reduce words to their stems, helping to simplify the vocabulary.\n",
    "   - **Lemmatization**: The `WordNetLemmatizer` from `nltk` is used to convert words into their base forms.\n",
    "\n",
    "3. **Vectorization**:\n",
    "   - **TF-IDF** (Term Frequency-Inverse Document Frequency): A TF-IDF matrix for the vocabulary is created using `TfidfVectorizer`, which helps to evaluate the importance of words in documents.\n",
    "   - **Bag of Words**: A frequency-based vector representation is created using `CountVectorizer`.\n",
    "\n",
    "4. **Topic Modeling (LDA)**:\n",
    "   - The `Latent Dirichlet Allocation` (LDA) method is employed for topic modeling to extract potential topics from the texts, displaying the top 10 most significant words for each topic.\n",
    "\n",
    "5. **Sentiment Analysis**:\n",
    "   - Sentiment analysis is performed on each text using `TextBlob` to compute sentiment polarity (ranging from -1 to 1), and the average sentiment score across all texts is calculated.\n",
    "\n",
    "6. **Document Similarity**:\n",
    "   - Document similarity is calculated using cosine similarity based on the TF-IDF matrix, identifying the most similar documents to a given document and listing the top five.\n",
    "\n",
    "7. **Keyword Extraction**:\n",
    "   - `TfidfVectorizer` is used again to extract keywords, and these are ranked based on their TF-IDF scores, displaying the top 10 keywords with the highest scores."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef4fdee9c00645be"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/a1234/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T17:02:19.364241Z",
     "start_time": "2024-04-22T17:02:18.954740Z"
    }
   },
   "id": "6e69430602eec71c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature names: ['00' '000' '0008fa19b63f4f2dbc68f1e3088caace' '000k' '000lb' '000lbs'\n",
      " '001' '001b61acf917474f9ff76a72c794cb70' '002' '003' '0036' '005'\n",
      " '005529' '007' '0076' '0077' '007788' '0083' '0084' '009' '00am' '00pm'\n",
      " '01' '01013' '01028' '01040' '0104241' '010455' '01060' '01075' '01077'\n",
      " '01085' '01089' '01109' '0111' '0111235' '0111248' '01129' '011636'\n",
      " '0118244' '011877c' '012331' '0125249' '0128ba4e5056a981' '012917' '013'\n",
      " '01301' '0131245' '013691' '01398ba45056a981']\n",
      "Bag of Words Feature names: ['00' '000' '0008fa19b63f4f2dbc68f1e3088caace' '000k' '000lb' '000lbs'\n",
      " '001' '001b61acf917474f9ff76a72c794cb70' '002' '003' '0036' '005'\n",
      " '005529' '007' '0076' '0077' '007788' '0083' '0084' '009' '00am' '00pm'\n",
      " '01' '01013' '01028' '01040' '0104241' '010455' '01060' '01075' '01077'\n",
      " '01085' '01089' '01109' '0111' '0111235' '0111248' '01129' '011636'\n",
      " '0118244' '011877c' '012331' '0125249' '0128ba4e5056a981' '012917' '013'\n",
      " '01301' '0131245' '013691' '01398ba45056a981']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Step 1: Load the data line by line\n",
    "file_path = '/Users/a1234/Desktop/BU/677 PYTHON/project/combined_data/combined_02134_Toyota_100_car_info.json'\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "# Step 2: Extract the 'Text' field\n",
    "texts = [item['Text'] for item in data]\n",
    "\n",
    "# Step 3: Preprocessing and NLP operations\n",
    "# Remove stopwords\n",
    "texts = [remove_stopwords(text) for text in texts]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "texts_stemmed = [' '.join([stemmer.stem(word) for word in text.split()]) for text in texts]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "texts_lemmatized = [' '.join([lemmatizer.lemmatize(word) for word in text.split()]) for text in texts]\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts_lemmatized)\n",
    "\n",
    "# Bag of Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(texts_lemmatized)\n",
    "\n",
    "# Print some results of TF-IDF and Bag of Words\n",
    "print(\"TF-IDF Feature names:\", tfidf_vectorizer.get_feature_names_out()[:50])  # Show some feature words\n",
    "print(\"Bag of Words Feature names:\", count_vectorizer.get_feature_names_out()[:50])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T17:27:40.729220Z",
     "start_time": "2024-04-22T17:27:33.800132Z"
    }
   },
   "id": "5b15d0d96bbea9fb"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: new, east, hartford, vehicle, haven, windsor, toyota, price, auto, best\n",
      "Topic 2: na, cab, diesel, 4x4, 2015, 4dr, sb, tacoma, dieselland, 603\n",
      "Topic 3: north, west, new, brookfield, 978, hadley, bridgewater, online, east, braintree\n",
      "Topic 4: toyota, new, power, with, clean, car, miles, rear, vehicle, great\n",
      "Topic 5: lease, own, blue, car, 7999, you, 6999, honda, need, weekly\n"
     ]
    }
   ],
   "source": [
    "'''Topic Modeling using Latent Dirichlet Allocation (LDA):'''\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Perform LDA\n",
    "lda = LatentDirichletAllocation(n_components=num_topics)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Get the topic-word distributions\n",
    "topic_word_distributions = lda.components_\n",
    "\n",
    "# Get the top words for each topic\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "n_top_words = 10\n",
    "for topic_idx, topic_words in enumerate(topic_word_distributions):\n",
    "    top_words = [feature_names[i] for i in topic_words.argsort()[:-n_top_words - 1:-1]]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T17:31:30.851136Z",
     "start_time": "2024-04-22T17:31:27.635412Z"
    }
   },
   "id": "e99ed2a9a5744b0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentiment: 0.16730260226571336\n"
     ]
    }
   ],
   "source": [
    "'''Sentiment Analysis using TextBlob:'''\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Perform sentiment analysis on each text\n",
    "sentiments = [TextBlob(text).sentiment.polarity for text in texts]\n",
    "\n",
    "# Print the average sentiment score\n",
    "avg_sentiment = sum(sentiments) / len(sentiments)\n",
    "print(\"Average Sentiment:\", avg_sentiment)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T17:31:45.221443Z",
     "start_time": "2024-04-22T17:31:42.569070Z"
    }
   },
   "id": "703540e826e2a85a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Document: Toyota 4 Runner TRD Pro Roof Rack - Fits 2010 - 2023 Models All mounting hardware included. Toyota OEM. 2 years old.\n",
      "Similar Document: 2 keys Toyota camry le 2002 Good condtion\n",
      "Similar Document: Sunroof , leather seats , Bluetooth , camera Clean title 2 owners 3 months warranty 2 original keys\n",
      "Similar Document: Fuel Injection Idle Air Control Valve, pulled good running 2001 Camry CE, good working condition, works Camry models, check number sure.\n",
      "Similar Document: keys Toyota camry le Also Mannuel 2002 Toyota camry le\n"
     ]
    }
   ],
   "source": [
    "'''Document Similarity using Cosine Similarity:'''\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate the cosine similarity between documents\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Get the most similar documents to a given document\n",
    "document_index = 0\n",
    "similar_documents = similarity_matrix[document_index].argsort()[::-1][1:6]  # Top 5 similar documents\n",
    "for doc_index in similar_documents:\n",
    "    print(\"Similar Document:\", texts[doc_index])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T17:31:56.115513Z",
     "start_time": "2024-04-22T17:31:55.765970Z"
    }
   },
   "id": "8d9bd557f214e1ae"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: na, TF-IDF Score: 378.0\n",
      "Keyword: toyota, TF-IDF Score: 122.93005684427112\n",
      "Keyword: new, TF-IDF Score: 94.66869604403516\n",
      "Keyword: cab, TF-IDF Score: 86.37138534506313\n",
      "Keyword: tacoma, TF-IDF Score: 77.5955236424751\n",
      "Keyword: 4x4, TF-IDF Score: 75.42622345376009\n",
      "Keyword: power, TF-IDF Score: 74.41829467892178\n",
      "Keyword: credit, TF-IDF Score: 72.25356678785614\n",
      "Keyword: 4dr, TF-IDF Score: 69.62573302668747\n",
      "Keyword: car, TF-IDF Score: 68.21537221762354\n"
     ]
    }
   ],
   "source": [
    "'''Keyword Extraction: '''\n",
    "# Extraction of keywords or phrases in the text, used to understand the theme or important content of the text\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed texts into TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts_lemmatized)\n",
    "\n",
    "# Get the feature names (keywords)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the TF-IDF scores for each keyword\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "# Sort the keywords based on their TF-IDF scores\n",
    "top_keywords_indices = tfidf_scores.argsort()[::-1][:10]  # Get top 10 keywords\n",
    "\n",
    "# Print the top keywords and their TF-IDF scores\n",
    "for index in top_keywords_indices:\n",
    "    keyword = feature_names[index]\n",
    "    tfidf_score = tfidf_scores[index]\n",
    "    print(f\"Keyword: {keyword}, TF-IDF Score: {tfidf_score}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T17:36:20.134797Z",
     "start_time": "2024-04-22T17:36:19.872215Z"
    }
   },
   "id": "5c9497bf5229cda2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
